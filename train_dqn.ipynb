{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories.time_step import time_step_spec,TimeStep,StepType\n",
    "from tf_agents.specs import BoundedArraySpec,tensor_spec\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"agent/util\")\n",
    "from simulation import BridgeAgent, EpisodeRunner,_BaseActionSolver,logger as simulation_logger\n",
    "from signal_state_util import WorldSignalState,SignalState,LaneVehicleNumCalc\n",
    "simulation_logger.setLevel(logging.ERROR)\n",
    "from road_tracer import RoadTracer\n",
    "from dnn_model import RunDistanceModel\n",
    "\n",
    "class _DenseOneSignalQNet():\n",
    "    def __init__(self,\n",
    "             num_states,\n",
    "             fc_layer_params = (100, 50, 30 , 20),\n",
    "        ):\n",
    "        self.num_actions=8\n",
    "        self.num_states=num_states\n",
    "        self.fc_layer_params=fc_layer_params\n",
    "        \n",
    "        self.time_step_spec=time_step_spec(tf.TensorSpec([self.num_states]))\n",
    "        self.action_spec=tensor_spec.from_spec(BoundedArraySpec((), np.int64,name=\"action\", minimum=0, maximum=self.num_actions-1))\n",
    "        self.random_policy = random_tf_policy.RandomTFPolicy(self.time_step_spec,self.action_spec)\n",
    "        \n",
    "    def _createNetwork(self):\n",
    "        dense_layers = [self.dense_layer(num_units) for num_units in self.fc_layer_params]\n",
    "        q_values_layer = tf.keras.layers.Dense(\n",
    "            self.num_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03),\n",
    "            bias_initializer=tf.keras.initializers.Constant(-0.2)\n",
    "        )\n",
    "        return sequential.Sequential(dense_layers + [q_values_layer])\n",
    "        \n",
    "    @staticmethod\n",
    "    def dense_layer(num_units):\n",
    "        return tf.keras.layers.Dense(\n",
    "            num_units,\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "                scale=2.0,\n",
    "                mode='fan_in',\n",
    "                distribution='truncated_normal')\n",
    "        )\n",
    "\n",
    "    def createDqnAgent(self,\n",
    "            learningRate,\n",
    "            targetUpdatePeriod=10,\n",
    "            epsilon=0.1,\n",
    "        ):\n",
    "        q_net=self._createNetwork()\n",
    "        \n",
    "        agent = dqn_agent.DqnAgent(\n",
    "            self.time_step_spec,\n",
    "            self.action_spec,\n",
    "            q_network=q_net,\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "            td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "            train_step_counter=tf.Variable(0),\n",
    "            target_update_period=targetUpdatePeriod,\n",
    "            epsilon_greedy=epsilon,\n",
    "        )\n",
    "        agent.initialize()\n",
    "        \n",
    "        # (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "        agent.train = common.function(agent.train)\n",
    "        \n",
    "        agent.train_step_counter.assign(0)\n",
    "        \n",
    "        return agent\n",
    "    \n",
    "    @staticmethod\n",
    "    def dense_layer(num_units):\n",
    "        return tf.keras.layers.Dense(\n",
    "            num_units,\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "                scale=2.0,\n",
    "                mode='fan_in',\n",
    "                distribution='truncated_normal')\n",
    "        )\n",
    "\n",
    "class EmbeddingOneSignalQNet(_DenseOneSignalQNet):\n",
    "    def __init__(self,\n",
    "            checkpointPath,\n",
    "            numSegmentInbound=9,\n",
    "            numSegmentOutbound=9,\n",
    "            segmentLength=25,\n",
    "            fc_layer_params = (100,50,30,20),\n",
    "        ):\n",
    "        self.segmentLength=segmentLength\n",
    "        \n",
    "        #embeddingは前処理として行う（Q学習における学習を行わず、重みは固定して使用する）\n",
    "        self.model=RunDistanceModel(\n",
    "                numSegmentInbound,\n",
    "                numSegmentOutbound,\n",
    "            )\n",
    "        self.model.load_weights(checkpointPath)\n",
    "        \n",
    "        numTimeState=1\n",
    "        numEmbeddingState=self.model.getEmbeddedVehiclesVecLength()\n",
    "        numPhaseProbState=8\n",
    "        numState=numTimeState+numEmbeddingState+numPhaseProbState\n",
    "        super().__init__(numState,fc_layer_params)\n",
    "        \n",
    "    def calcState(self,tracer,interId,current_time):\n",
    "        _embeddedVec,_prob = self.model.calcEmbeddingVec(tracer,interId,self.segmentLength)\n",
    "        \n",
    "        state=np.concatenate([\n",
    "            np.array([observations.current_time]), #shape = [1] \n",
    "            _embeddedVec.mean(axis=0), #shape = [Dv] \n",
    "            _prob, #shape = [8] \n",
    "        ]) #shape = [Dv+9] \n",
    "        return state\n",
    "    \n",
    "class LaneVehicleNumDenseOneSignalQNet(_DenseOneSignalQNet):\n",
    "    def __init__(self,\n",
    "            fc_layer_params = (100, 50, 30 , 20),\n",
    "        ):\n",
    "        numTimeState=1\n",
    "        numLaneState=12\n",
    "        numPhaseState=8\n",
    "        numState=numTimeState+numLaneState+numPhaseState\n",
    "        super().__init__(numState,fc_layer_params)\n",
    "        \n",
    "    def calcState(self,tracer,interId,current_time):\n",
    "        timeState=[current_time]\n",
    "        \n",
    "        #公式のlane_vehicle_numは、「正しいレーンにレーン移動する前の車両」を含むため誤りの原因になるので使用しない\n",
    "        #代わりにroute情報から計算したlane_vehicle_numを使用\n",
    "        laneState=tracer.calcNumVehicleOnLane(interId)\n",
    "        assert(len(laneState)==12)\n",
    "        \n",
    "        _signalState=tracer.worldSignalState.signalStateDict[interId]\n",
    "        phaseState=_signalState.getPassableEncodingWithoutRightTurn()\n",
    "        assert(len(phaseState)==8)\n",
    "        state=timeState+laneState+phaseState\n",
    "        return state\n",
    "\n",
    "class SegmentedLaneVehicleNumAndSpeedDenseOneSignalQNet(_DenseOneSignalQNet):\n",
    "    def __init__(self,\n",
    "            fc_layer_params = (100, 50, 30 , 20),\n",
    "            numSegment=5,\n",
    "            segmentLength=25,\n",
    "        ):\n",
    "        self.segmentLength=segmentLength\n",
    "        self.numSegment=numSegment\n",
    "        \n",
    "        numTimeState=1\n",
    "        numLaneVehicleNumState=12*numSegment\n",
    "        numLaneVehicleSpeedState=12*numSegment\n",
    "        numPhaseState=8\n",
    "        numState=numTimeState+numLaneVehicleNumState+numLaneVehicleSpeedState+numPhaseState\n",
    "        super().__init__(numState,fc_layer_params)\n",
    "        \n",
    "    def calcState(self,tracer,interId,current_time):\n",
    "        timeState=[current_time]\n",
    "\n",
    "        laneVehicleNumState,laneVehicleSpeedState=tracer.calcVehicleNumAndSpeedOnSegmentedInboundLane(\n",
    "            interId,\n",
    "            self.numSegment,\n",
    "            self.segmentLength\n",
    "        )\n",
    "                \n",
    "        _signalState=tracer.worldSignalState.signalStateDict[interId]\n",
    "        phaseState=_signalState.getPassableEncodingWithoutRightTurn()\n",
    "        assert(len(phaseState)==8)\n",
    "        state=timeState+laneVehicleNumState+laneVehicleSpeedState+phaseState\n",
    "        return state\n",
    "    \n",
    "class DebugPrinter:\n",
    "    def __init__(self,agent):\n",
    "        self.agent=agent\n",
    "        self.enablePrint=True\n",
    "        self.prohibitPrintActionDicision=False\n",
    "        self.prohibitPrintReward=False\n",
    "        self.prohibitPrintLoss=False\n",
    "        \n",
    "        self.printLossIntervalStep=100\n",
    "        self.trainResultList=[]\n",
    "        \n",
    "    def printActionDicision(self,timeStep,select_phase,policyName,current_time):\n",
    "        if self.enablePrint and not self.prohibitPrintActionDicision:\n",
    "            step_type=timeStep.step_type.numpy()[0]\n",
    "            if step_type!=StepType.LAST:\n",
    "                print(\"****************** action dicision: time={} *****************\".format(current_time))\n",
    "                if policyName!=\"random_tf_policy\":\n",
    "                    print(\"---q network input----------\")\n",
    "                    print(\"observation\",timeStep.observation.numpy())\n",
    "                    print(\"---q network output-----\")\n",
    "                    outputs,_=self.agent._q_network(timeStep.observation,step_type=timeStep.step_type)\n",
    "                    print(\"q_value\",outputs.numpy())\n",
    "                print(\"---dicision----------\")\n",
    "                print(\"{} : select_phase={}\".format(policyName,select_phase))\n",
    "                print(\"-------------\")\n",
    "\n",
    "    def printReward(self,last_select_phase,reward,observations,roadNet):\n",
    "        if self.enablePrint and not self.prohibitPrintActionDicision:\n",
    "            current_time=observations.current_time\n",
    "            delayIndex=observations.vehicleDS.calcDelayIndex(roadNet.roadDataSet)\n",
    "            print(\"****************** reward: time={} *****************\".format(current_time))\n",
    "            print(\"reward={:.3}, delayIndex={:.3}, last_select_phase={}\".format(\n",
    "                reward,\n",
    "                float(delayIndex),\n",
    "                last_select_phase\n",
    "            ))\n",
    "            print(\"--------\")\n",
    "            \n",
    "    def printLoss(self,step,train_loss):\n",
    "        self.trainResultList.append((step,train_loss))\n",
    "        if self.enablePrint and not self.prohibitPrintLoss:\n",
    "            if step % self.printLossIntervalStep == 0:\n",
    "                print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "class RewardCalc:\n",
    "    def __init__(self,rewardType=\"di\"):\n",
    "        self.rewardType=rewardType\n",
    "    def calc(self,observations,interId,roadNet):\n",
    "        if self.rewardType==\"di\":\n",
    "            di=observations.vehicleDS.calcDelayIndex(roadNet.roadDataSet)\n",
    "            return 1/di\n",
    "        elif self.rewardType==\"inbound_out\" or self.rewardType==\"into_outbound\":\n",
    "            if observations.rewards is not None:\n",
    "                arrInOut=np.array(observations.rewards[interId])\n",
    "                idx1=1 if rewardType==\"inbound_out\" else 0\n",
    "                idx0=0 if rewardType==\"inbound_out\" else 12\n",
    "                return arrInOut[idx0:idx0+12,idx1].sum()/30\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            raise Exception(\"not expected\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,agent,replayBufferMaxLength,batchSize):\n",
    "        self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            data_spec=agent.collect_data_spec,\n",
    "            batch_size=1,\n",
    "            max_length=replayBufferMaxLength\n",
    "        )\n",
    "        self.dataset = self.replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3, \n",
    "            sample_batch_size=batchSize, \n",
    "            num_steps=2,\n",
    "        ).prefetch(3)\n",
    "        self.iterator=iter(self.dataset)\n",
    "    def getNextExperience(self):\n",
    "        ex,_=next(self.iterator)        \n",
    "        return ex\n",
    "    def add(self,traj):\n",
    "        self.replay_buffer.add_batch(traj)\n",
    "    def getLength(self):\n",
    "        return self.replay_buffer.num_frames().numpy()\n",
    "    \n",
    "class SignalDQNActionSolver(_BaseActionSolver):\n",
    "    # 一つの信号ごとにその最適なフェーズ（を決めるQ値）を出力するDQNを実行する。\n",
    "    # 都市にある異なる信号でDQNは共有され、同じDQNで学習や推論を行う。\n",
    "    def __init__(self,\n",
    "            qNet,\n",
    "            learningRate=1e-3,\n",
    "            targetUpdatePeriod=10,\n",
    "            epsilon=0.1,\n",
    "            discount=0.9,\n",
    "            replayBufferMaxLength = 320,\n",
    "            batchSize=64,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.discount=discount\n",
    "        self.qNet=qNet\n",
    "        self.agent = qNet.createDqnAgent(\n",
    "            learningRate,\n",
    "            targetUpdatePeriod,\n",
    "            epsilon\n",
    "        )\n",
    "        self.debugPrinter=DebugPrinter(self.agent)\n",
    "        self.rewardCalc=RewardCalc(\"di\")\n",
    "        \n",
    "        self.replayBufferMaxLength=replayBufferMaxLength\n",
    "        self.batchSize=batchSize\n",
    "        self.interIdToBufDict={}\n",
    "\n",
    "    def startFirstEpisode(self,signalizedInterIdList):\n",
    "        #episodeを超えて存在\n",
    "        for interId in signalizedInterIdList:\n",
    "            self.interIdToBufDict[interId] = ReplayBuffer(\n",
    "                self.agent,\n",
    "                self.replayBufferMaxLength,\n",
    "                self.batchSize\n",
    "            )\n",
    "        \n",
    "    def getTrainProgress(self):\n",
    "        li=self.debugPrinter.trainResultList\n",
    "        if len(li)>=1:\n",
    "            trainStep,trainLoss = li[-1]\n",
    "        else:\n",
    "            trainStep=0\n",
    "            trainLoss=0.\n",
    "        return trainStep,trainLoss\n",
    "    \n",
    "    def getBufferLength(self):\n",
    "        if len(self.interIdToBufDict) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            for buf in self.interIdToBufDict.values():\n",
    "                #return first buffer length (each buf has same length)\n",
    "                return buf.getLength()\n",
    "    \n",
    "    def _createTimeStep(self,observations,interId,stepType,exitType):\n",
    "        tracer=self._createRoadTracer(observations)\n",
    "        state=self.qNet.calcState(tracer,interId,observations.current_time)\n",
    "        \n",
    "        extra_reward = self.rewardCalc.calc(observations,interId,self.roadNet)\n",
    "        assert(0<=extra_reward and extra_reward<=1)\n",
    "        reward=extra_reward if stepType==StepType.MID else 0\n",
    "        discount = self.discount if stepType!=StepType.LAST else 0\n",
    "        \n",
    "        if stepType!=StepType.FIRST:\n",
    "            signalState=self.worldSignalState.signalStateDict[interId]\n",
    "            self.debugPrinter.printReward(\n",
    "                signalState.prevPolicyStepForDQN.action.numpy()[0]+1,\n",
    "                reward,\n",
    "                observations,\n",
    "                self.roadNet,\n",
    "            )\n",
    "        \n",
    "        return TimeStep(\n",
    "            tf.constant([stepType]),\n",
    "            tf.constant([reward],dtype=tf.float32),\n",
    "            tf.constant([discount],dtype=tf.float32),\n",
    "            tf.constant([state],dtype=tf.float32)\n",
    "        )\n",
    "    \n",
    "    def _collect(self,stepType,interId,currentTimeStep):\n",
    "        if stepType==StepType.FIRST:\n",
    "            if self.lastEpisodeWorldSignalState is not None:\n",
    "                lastSignalState=self.lastEpisodeWorldSignalState.signalStateDict[interId]\n",
    "            else:\n",
    "                lastSignalState = None\n",
    "        else:\n",
    "            lastSignalState=self.worldSignalState.signalStateDict[interId]\n",
    "\n",
    "        if lastSignalState is not None:\n",
    "            # save to the replay buffer.\n",
    "            traj=trajectory.from_transition(\n",
    "                lastSignalState.prevTimeStepForDQN,\n",
    "                lastSignalState.prevPolicyStepForDQN,\n",
    "                currentTimeStep\n",
    "            )\n",
    "            self.interIdToBufDict[interId].add(traj)\n",
    "\n",
    "    def _train(self,interId):\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience = self.interIdToBufDict[interId].getNextExperience()\n",
    "        train_loss = self.agent.train(experience).loss\n",
    "        self.debugPrinter.printLoss(\n",
    "            self.agent.train_step_counter.numpy(),\n",
    "            train_loss.numpy()\n",
    "        )\n",
    "        \n",
    "    def _action(self,timeStep,signalState,policy,current_time):\n",
    "        policyStep = policy.action(timeStep)\n",
    "        select_phase=policyStep.action.numpy()[0]+1\n",
    "        \n",
    "        signalState.setPreviousStepInfo(\n",
    "            prevTimeStepForDQN=timeStep,\n",
    "            prevPolicyStepForDQN=policyStep,\n",
    "        )\n",
    "        \n",
    "        self.debugPrinter.printActionDicision(\n",
    "            timeStep,\n",
    "            select_phase,\n",
    "            policy.name,\n",
    "            current_time\n",
    "        )\n",
    "        \n",
    "        return select_phase\n",
    "    \n",
    "    def decideActions(self,\n",
    "            observations,\n",
    "            prevActCountInEpisode,\n",
    "            runType=\"eval\",\n",
    "            exitType=False,\n",
    "            debug=False\n",
    "        ):\n",
    "        if runType==\"eval\":\n",
    "            policy = self.agent.policy\n",
    "            collect=False\n",
    "            train=False\n",
    "            self.debugPrinter.enablePrint=True and debug\n",
    "        elif runType==\"train\":\n",
    "            policy = self.agent.collect_policy\n",
    "            collect=True\n",
    "            train=True\n",
    "            self.debugPrinter.enablePrint=False and debug\n",
    "        elif runType==\"random\":\n",
    "            policy = self.qNet.random_policy\n",
    "            collect=True\n",
    "            train=False\n",
    "            self.debugPrinter.enablePrint=False and debug\n",
    "        else:\n",
    "            raise Exception(\"not expected\")\n",
    "        \n",
    "        if prevActCountInEpisode==0:\n",
    "            stepType = StepType.FIRST \n",
    "        else:\n",
    "            stepType = StepType.LAST if exitType is not None else StepType.MID\n",
    "        \n",
    "        actions={}\n",
    "        for interId,signalState in self.worldSignalState.signalStateDict.items():\n",
    "            #episode's FIRST or MID or LAST step\n",
    "            \n",
    "            #convert observations into input for Q-netowrk\n",
    "            currentTimeStep=self._createTimeStep(observations,interId,stepType,exitType)\n",
    "            \n",
    "            if collect:\n",
    "                self._collect(stepType,interId,currentTimeStep)\n",
    "                \n",
    "            select_phase=self._action(currentTimeStep,signalState,policy,observations.current_time) #need call even if step is LAST for saving last action for next episode\n",
    "            \n",
    "            if stepType!=StepType.LAST:\n",
    "                if select_phase != signalState.phase:\n",
    "                    signalState.changePhase(select_phase,observations.current_time)\n",
    "                    actions[interId]=select_phase\n",
    "                    \n",
    "        if stepType!=StepType.FIRST and train:\n",
    "            for interId in self.worldSignalState.signalStateDict:\n",
    "                self._train(interId)\n",
    "            \n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3341db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "simulator_cfg_file = \"cfg/simulator_test1.cfg\"\n",
    "metric_period = 1\n",
    "initialBufferLength=100\n",
    "numTrainEpisode = 1000\n",
    "earlyStoppingDelayIndex=1.6\n",
    "\n",
    "embeddingWeightPath=\"ckpt/test2_9_9_10000_1_1.ckpt\"\n",
    "\n",
    "DEBUG=False\n",
    "#####################################\n",
    "\n",
    "#dqnRunner=SignalDQNActionSolver(SegmentedLaneVehicleNumAndSpeedDenseOneSignalQNet())\n",
    "dqnRunner=SignalDQNActionSolver(EmbeddingOneSignalQNet(embeddingWeightPath))\n",
    "runner=EpisodeRunner(dqnRunner,simulator_cfg_file,metric_period,debug=DEBUG)\n",
    "\n",
    "            \n",
    "print(\"collecting experience using random policy\")\n",
    "runner.runLoop(\"random\",earlyStoppingDelayIndex,breakReplayBufferLength=initialBufferLength,tqdm=tqdm)\n",
    "\n",
    "print(\"training with collecting experience using dqn epsilon-greedy policy\")\n",
    "runner.runLoop(\"train\",earlyStoppingDelayIndex,breakNumEpsiode=numTrainEpisode,eval_every_n_episode=10,tqdm=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "simulator_cfg_file = \"cfg/simulator_test2.cfg\"\n",
    "metric_period = 1\n",
    "initialBufferLength=100\n",
    "numTrainEpisode = 1000\n",
    "earlyStoppingDelayIndex=1.6\n",
    "\n",
    "embeddingWeightPath=\"ckpt/test2_9_9_10000_1_1.ckpt\"\n",
    "\n",
    "DEBUG=False\n",
    "#####################################\n",
    "\n",
    "#dqnRunner=SignalDQNActionSolver(LaneVehicleNumDenseOneSignalQNet())\n",
    "dqnRunner=SignalDQNActionSolver(EmbeddingOneSignalQNet(embeddingWeightPath))\n",
    "runner=EpisodeRunner(dqnRunner,simulator_cfg_file,metric_period,debug=DEBUG)\n",
    "\n",
    "            \n",
    "print(\"collecting experience using random policy\")\n",
    "runner.runLoop(\"random\",earlyStoppingDelayIndex,breakReplayBufferLength=initialBufferLength,tqdm=tqdm)\n",
    "\n",
    "print(\"training with collecting experience using dqn epsilon-greedy policy\")\n",
    "runner.runLoop(\"train\",earlyStoppingDelayIndex,breakNumEpsiode=numTrainEpisode,eval_every_n_episode=10,tqdm=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b40551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################\n",
    "# simulator_cfg_file = \"cfg/simulator_warm_up.cfg\"\n",
    "# metric_period = 1\n",
    "# initialBufferLength=100\n",
    "# numTrainEpisode = 1000\n",
    "# earlyStoppingDelayIndex=1.3\n",
    "\n",
    "# DEBUG=False\n",
    "# #####################################\n",
    "\n",
    "# dqnRunner=SignalDQNActionSolver(LaneVehicleNumDenseOneSignalQNet())\n",
    "# runner=EpisodeRunner(dqnRunner,simulator_cfg_file,metric_period,debug=DEBUG)\n",
    "\n",
    "            \n",
    "# print(\"collecting experience using random policy\")\n",
    "# runner.runLoop(\"random\",earlyStoppingDelayIndex,breakReplayBufferLength=initialBufferLength,tqdm=tqdm)\n",
    "\n",
    "# print(\"training with collecting experience using dqn epsilon-greedy policy\")\n",
    "# runner.runLoop(\"train\",earlyStoppingDelayIndex,breakNumEpsiode=numTrainEpisode,eval_every_n_episode=10,tqdm=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1122ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(runner.evalRecordList,columns=['time','served','DI','train','loss'])\n",
    "df[['DI','loss']].plot()\n",
    "df[['time']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.runLoop(\"eval\",tqdm=tqdm)\n",
    "runner.export(tqdm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
